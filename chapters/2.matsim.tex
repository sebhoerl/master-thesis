\section{Simulation Framework}
\label{sec:matsim}

The investigations of autonomous vehicles in this thesis will be based on the
agent-based transport simulation MATSim \citep{Horni2015}. The following sections
will outline how the framework works and where it can be extended to shape it
towards an autonomous taxi simulation. An overview will be given on which components
need to be modified and how the final transport situation will result from all
the different parts that make up the simulation.

\subsection{Agent-based transport simulation}

The approach that is used in MATSim is to simulate a virtual population of a city on a per-agent
timestep-based level. At the beginning of a simulation run, each person in the synthetic
population has an initial plan of what it is supposed to do during the simulated day. Those
 plans consist of two elements:

\begin{description}

\item[Activities] have a start and an end time, as well as, depending on the
respective scenario, certain constraints on when the earliest start or latest
end time could be, i.e. how much the current values could be altered to still
give a realistic time frame. Furthermore, minimum durations can be defined for
which an agent has to stay at a certain activity location. Those locations are
given as facilities, which have specific coordinates on the scenario map.
Usual activities are
``home'' (which usually is the first plan element of a day) and ``work''. More
elaborate simulations can additionally use an arbitrary number of secondary
activities.

\item[Legs] are the second type of plan elements. These describe connections
between two activity locations and contain information like which mode of transport
the agent will use (e.g. ``car'', ``public transport'', or ``walk'') and, depending
on the selected mode, further data like the route that should be taken through
the street network.

\end{description}

A typical day plan of a MATSim agent can be seen in \cref{fig:typical_plan}. The
agent starts at home, then walks to his job, stays there for a certain time and then
goes back home. Each agent in a popuation (which can range from several hundred to
hundeds of thousands) has its individual plan that is executed when one day
is simulated.

\begin{figure}
    \centering
    \includegraphics[width=0.8\textwidth]{figures/simpleplan.pdf}
    \caption{A typical agent plan in MATSim}
    \label{fig:typical_plan}
\end{figure}

The network, on which the simulation is taking place, is described through nodes
and connecting links. These are defined by a specific flow capacity which tells the
simulation how many vehicles are able to pass the link within a certain time
frame while the length and an average link speed determine how fast vehicles
will travel to the next node.

The whole MATSim simulation is performed time-step by time-step. In a single simulation step (usually 1 second)
the agents that are currently in an activity do not need to be taken into account unless their scheduled
activity end is reached. The other agents, which are currently on a leg, are simulated in a
dedicated traffic network simulation. This simulation moves the agents according
to the capacity and current congestion from the start node to the end node of the
respective links. Traffic is not simulated on a micro-level (i.e. the vehicles
do not have distinct positions along the link) to increase the simulation
speed. Consequently, congestion on the traffic network is emerging from the travel
plans of all the agents. If there are too many agents who try to use one route
at the same time, the overall congestion will increase.

The actual routes that are being taken by the agents are generated based on the
generalized least cost path through the network for a certain leg, with a certain amount stochasticity
added into the pathfinding process in order to avoid artificially created bottlenecks.
This approach allows the optimization of the plans to be separated from the actual
simulation of the plans, but is not suitable for the simulation of an AV taxi service,
where routes through the network must be generated dynamically, based on the current
demand. Therefore, to simulate dynamic agents for AVs, which are not statically residing in a
fixed-timed activity or a predefined leg route, significant changes need
to be made, and some of the computational advantages of the simulation approach
need to be partly circumvented. This, however, mainly addresses implementational
details and will be discussed later in the thesis in \cref{sec:dynagent}.

All steps described above, i.e. the simulation of activities and legs during
a whole simulation day, are summarized as the ``Mobility Simulation'' or, in short, Mobsim
of the MATSim framework.

\subsection{Utility-based scoring}
\label{sec:utility}

As pointed out in the previous section, individual travel decisions might lead to
waiting times on the network, which then can also lead to late arrivals at the
designated activity locations, which usually would be disadvantageous in the real
world. Therefore, it might be beneficial for an agent to reconsider the time and
route choices that had been made for the current day, just as a real person
would do.

In order to ``know'' whether a plan worked out well or was disadvantageous, the
single elements need to be weighed and quantified. This is done using the Charypar-Nagel
scoring function \citep[pp. 27]{Horni2015}:


\begin{equation}
S_{plan} = \sum_{q=0}^{N-1} S_{act,q} + \sum_{q=0}^{N-1} S_{trav,mode(q)}
\end{equation}

It combines the marginal utilities of all activities ($S_{act,q}$) ranging
over the $N$ activities in a plan and the marginal utilities for all the legs in
between.

The marginal utility for the activities, among other factors, depends on how long
the activity has been performed, whether the agent needed to wait to start it (due
to an early arrival) or whether it was forced to leave early. For more details
the complete computation is shown in \citet{Horni2015}.

For the scope of this thesis the travel utility is more interesting. A basic version
for a single leg $q$ can look as follows:
\begin{equation}\begin{aligned}
S_{trav,mode(q)} = C_{mode(q)} + \beta_{trav,mode(q)} \cdot t_{trav,q} + \gamma_{d,mode(q)} \cdot \beta_{m} \cdot d_{trav,q}
\end{aligned}\end{equation}

\begin{description}

\item[Mode Choice] The first term, $C_{mode(q)}$, describes a constant (dis)utility for the
choosing a certain mode for the leg. It can be interpreted as how ``favorable''
going on a trip in the specific mode is and is negative. An classic use of the parameter
is to model constant costs per trip in a mode.

\item[Travel Time] The parameter $\beta_{trav,mode(q)}$ is the marginal utility of traveling,
which is multiplied by the time spent on the leg $t_{trav,q}$. It signifies how
favorable it is to spend time on such a leg, i.e. the longer one needs to stay
in a car or public transport, the larger the disutility gets (and therefore
the parameter is usually negative). Values which are smaller, therefore,
stand for travel modes where time is spent less useful or comfortably. The unit is
``utility per time''.

\item[Travel Cost] The third element involves the (positive) marginal utility of money $\beta_{m}$,
which is a universal simulation parameter and describes how the utility of money can
be weighed against e.g. time., the unit being ``utility per monetary unit'', e.g. EUR.
It is multiplied by the (negative) monetary distance rate
$\gamma_{d,mode(q)}$, which states, to how much disutility per spanned
distance the leg will lead. For the given example it would be stated as ``EUR per meter''.
The parameter is useful for imposing distance-based
fares in a transport mode and thus making it monetarily attractive or
unattractive compared to other ways of traveling.

\end{description}

Beyond these parameters, which are usually used for all travel modes, there are
a number of additions, e.g. for public transport, or yet unused options, such
as a direct marginal utility of distance traveled.

For the purpose of simulation autonomous taxi services, one addition is made:
\begin{equation}\begin{aligned}
S_{av} &= C + \beta_{trav,av} \cdot t_{trav} + \gamma_{d,av} \cdot \beta_m \cdot d_{trav} + \beta_{wait,av} \cdot t_{wait}
\end{aligned}\end{equation}

Here, $\beta_{wait}$ is the marginal utility of waiting time, quantifying
how disadvantageous it is to wait for an AV to arrive.

The utility computations presented here are used in the ``scoring phase'', which
is taking place in MATSim after one simulation day\footnote{A simulated day in
MATSim can be defined to be longer than 24 hours to ensure that all agents departing
late at night are given enough time to arrive at their destinations, and thus not
be penalized for having infinite travel time.} has been performed. Then all experienced legs and activities are scored, and
each agent is assigned a final score, depending on which further replanning is done,
as described in the next part.

\subsection{Evolutionary replanning}

The last step is to make all the agents replan their day
in order to ``learn'' more optimal plans which
ensure that they arrive on time at the activity locations and avoid congestion.
This is done using an evolutionary algorithm that works as follows:

Usually, an agent will start out with one quite random plan, go through it during
one whole day and then get a score for it. Afterward, if the agent is selected in a random draw, it's plan is
copied and modified slightly. Those modifications can happen with respect
to start and end times of activities, mode choices for certain legs, etc. So after
one iteration the agent might already have two plans to select.

Before the next day starts, one of the available plans is selected according to a certain
strategy. The standard approach is to do a multinomial selection with respect to
the previously obtained plan scores. So one after another plans, will be created,
scored, modified, rescored and so on. Because of the selection process, which
favors high scores, better and better choices will be made.

However, this is done for each and every agent, so while improving the performance
of one agent's plans, this might effect the performance of other agents negatively,
which is especially true if one thinks about the example of highly congested roads
due to too many agents choosing the same route. Finally, though, the algorithm reaches
a quasi-equilibrium, which in MATSim is usually referred to as the ``relaxed
state'', in which the average score of the used plans stabilizes within a
reasonable variance.

Each ``day'' that is simulated in this manner is usually called an ``iteration''
of the simulation. It is common to divide these iterations into two parts. The
first one is the ``innovation phase'', where plans have a certain probability to
be modified while innovation is turned off in the second phase. This means that
the agent will only choose among the present plans in his repertoire (usually
around 5) and rescore them again and again, until the most favorable is selected.

All of the above is known as the ``replanning'' in MATSim. Putting everything
together, a whole cycle in a MATSim simulation can be seen in \cref{fig:matsimcycle}. Since the
final traffic situation evolves from the evolutionary choice in the replanning and
selection, as well as from the emergent congestion in the Mobsim, this whole
cycle is usually referred to as a ``co-evolutionary'' algorithm.

\Cref{fig:scorestats} shows a typical progression of the population-wide score in
a MATSim simulation. What one can see there is an average of the worst and best plans
of each agent. Additionally the mean of the per-agent average scores in their list of plans is displayed.
Finally, one can see the average of all the plans that have been executed
by the agents in a particular iteration.

The first phase until iteration 100 is the innovation phase, where time and mode
choices can be made, following which it is turned off at iteration 100. At this point,
all agents select from their existing stock of plans, stochastically favoring higher
scores. What was a high-scoring plan during the innovation phase might now perform substantially
worse, while another plan in each agent's choice set might produce better scores under these newly
imposed conditions. This change is reflected in the lowering of the average maximum
plan score and increase of average executed score after 100 iterations in \cref{fig:scorestats}.
Finally, a quite stable population-wide relaxed state is reached.

\begin{figure}
    \centering
    \includegraphics[width=1.0\textwidth]{figures/matsimcycle.pdf}
    \caption{The basic co-evolutionary algorithm of MATSim, showing the three main
    stages: Mobsim, Scoring and Replanning/Selection}
    \label{fig:matsimcycle}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.9\textwidth]{figures/scorestats.pdf}
    \caption{Typical progression of the agent scores throughout a MATSim simulation.
    ``Worst'' means that the plans with the worst scores from all the agents are taken
    and averaged, the same applies for the other graphs.}
    \label{fig:scorestats}
\end{figure}

\subsection{Queue-based simulation}

The heart of the simulation in MATSim is the queue simulation (QSim), which is a
specific implementation of the beforementioned Mobsim. This part of the framework
is iterating through all the agents that need to take action in the current simulation step.

Itself, the QSim is split into the handling of agents which are currently performing
an activity (i.e. are in an idle state in terms of the traffic simulation) and another
part, the Netsim, which is simulating the traffic network.

When running the Netsim, the agents are moved on the network according to their
current route and dependent on congestion. After moving an agent, the Netsim checks
whether the agent should end its trip at the current link to which he has been moved. If
this is the case, the agent is removed from the Netsim and the next agent state is computed.

The result of this computation is either that the agent wants to start a leg, in
which case he is reinserted into the Netsim, or that he wants to start an activity,
which will add the agent to the activity queue.

This activity queue is the other main component of the QSim. In fact, it is a
priority queue, where all agents are sorted according to the time at which they
want to end the next activity. So when adding an agent to the activity queue, it first
checks when the activity should be ended and then the agent is inserted at the
corresponding position in the queue.

The processing of this queue in the QSim for each simulation step then works as
follows: The first element of the queue is checked, whether the
agent should already end the activity. If this is not the case, the simulation step
is already finished. On the other hand, if the activity should be ended now (or
previously if the time resolution of the simulation is quite high), the agent is
removed from the top of the queue and the next state is computed as described above.
Then the new top element of the queue is examined.
The whole QSim simulation is schematically rendered in \cref{fig:qsim}.

The big advantage of this simulation architecture is as follows: When an agent is
in an activity, no computation needs to be performed. So instead of polling all
the agents in every simulation step to check if they want to end an activity, the
computational demand is much lower when using the queue, since many agents can
be skipped. The sequential processing of the priority queue is very fast in terms
of computational complexity ($\mathcal{O}(1)$ for checking the top element
and $\mathcal{O}(\log n)$ for fetching it) \citep{JavaPQ}.

Furthermore, the same concept is used within the Netsim to speed up the computation
of the traffic situation. In both cases, for the QSim and Netsim, those savings in
computation time naturally decrease the versatility of the simulation environment.

One major drawback is that if an agent, which is already queued, should abort its
current activity, it needs to be removed from the queue and added at a new position.
Both operations are quite costly for the priority queue \citep{JavaPQ}, so
if more and more reschedulings are needed, the computational overhead can become
quite large.

However, for truly dynamic agents, like autonomous vehicles, it is necessary to
adopt their plans frequently and thus some thought needs to be put into how to
achieve this freedom while still keeping as many advantages from the existing
simulation architecture. \Cref{sec:dynagent} will explain how this problem has
been overcome in this thesis.

\begin{sidewaysfigure}
    \centering
    \includegraphics[width=0.9\textwidth]{figures/qsim.pdf}
    \caption{Simplified structure of the QSim in MATSim.}
    \label{fig:qsim}
\end{sidewaysfigure}
